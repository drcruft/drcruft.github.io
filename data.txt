1

Here’s the quick rundown on **DeepSeek-V3.1**.

**Parameter sizes**

* **671B total parameters, \~37B active per token** (MoE). Same as V3.&#x20;
* Architecture unchanged from V3: **DeepSeekMoE** + **Multi-head Latent Attention (MLA)**; each MoE layer has **1 shared + 256 routed experts**, with **top-8** experts active per token.&#x20;

**Context & modes**

* **128K context window** for both the “chat” (non-thinking) and “reasoner” (thinking) variants. ([DeepSeek API Docs][1])
* **Hybrid “Think / Non-Think” in one model**—you switch modes via the chat template; V3.1-Think is faster than R1-0528 for reaching answers. ([DeepSeek API Docs][2], [Hugging Face][3])

**What’s new/featured in V3.1**

* **Improved tool/agent skills** (better SWE/Terminal-Bench, multi-step tasks); **faster thinking efficiency**. ([DeepSeek API Docs][2])
* **Function calling**: supported in **non-thinking** (“deepseek-chat”) mode; **JSON output** supported in both modes. ([DeepSeek API Docs][1])
* **Two-phase long-context training** on top of V3: \~**630B tokens** for the 32K phase and **209B tokens** for the 128K phase; updated tokenizer & chat template. ([Hugging Face][3])
* **FP8 microscaling (UE8M0)** for weights & activations to improve efficiency/compatibility. ([Hugging Face][3])

**Carry-overs from V3 you still get in V3.1**

* **Auxiliary-loss-free load balancing** and **multi-token prediction** (used to speed up decoding), plus FP8 support in training.&#x20;

If you want, tell me your use case (chatbot, agent, long-doc QA, coding, etc.) and I’ll map you to the right mode + features.

[1]: https://api-docs.deepseek.com/quick_start/pricing "Models & Pricing | DeepSeek API Docs"
[2]: https://api-docs.deepseek.com/news/news250821 "DeepSeek-V3.1 Release | DeepSeek API Docs"
[3]: https://huggingface.co/deepseek-ai/DeepSeek-V3.1 "deepseek-ai/DeepSeek-V3.1 · Hugging Face"


Here’s the skinny on **OpenAI gpt-oss-120b** (aka “GPT-OSS 120B”):

* **Size & architecture**

  * **117B total params** (marketed as “120B”), **Mixture-of-Experts** transformer. **128 experts/layer**, **4 active per token** → about **5.1B active params** per token. **36 layers.** ([OpenAI][1])
  * **Attention & pos. enc.**: alternating dense + locally banded sparse attention, **grouped multi-query attention (group=8)**, **RoPE**. ([OpenAI][1])
  * **Context window:** **128K tokens.** ([OpenAI][1])

* **Tokenizer & format**

  * Uses the **o200k\_harmony** tokenizer and expects the **harmony response format**. ([OpenAI][1])

* **Deployment & memory**

  * **Native MXFP4 quantization**; runs on a **single 80 GB GPU** (e.g., H100/MI300X). Weights available on **Hugging Face**. ([OpenAI][1], [Hugging Face][2])

* **License**

  * **Apache-2.0** open-weight release. ([OpenAI][1], [Hugging Face][2])

* **Features**

  * **Reasoning levels** (low/medium/high) to trade off speed vs. accuracy. ([OpenAI][1], [Hugging Face][2])
  * **Tool use / agentic skills**: function calling, structured outputs, web browsing, and Python tool execution supported in the provided tool environments. ([OpenAI][1], [Hugging Face][2])
  * **Chain-of-thought access** for developers (OpenAI advises not to show CoT to end-users). ([OpenAI][1], [Hugging Face][2])
  * **Fine-tuning:** supported; **120B can be fine-tuned on a single H100 node.** ([Hugging Face][2])

If you tell me your use case (local inference, RAG, agents, etc.), I can suggest practical configs (reasoning level, quantization, context settings) for gpt-oss-120b.

[1]: https://openai.com/index/introducing-gpt-oss/ "Introducing gpt-oss | OpenAI"
[2]: https://huggingface.co/openai/gpt-oss-120b "openai/gpt-oss-120b · Hugging Face"


Here’s the essentials on **Meta Llama 4 “Maverick”**.

**Parameter sizes (MoE)**

* **17B activated params** per token, **\~400B total** across **128 experts** (Mixture-of-Experts). ([Hugging Face][1], [Amazon Web Services, Inc.][2])

**Context window (model-native vs. host limits)**

* Model-native: **up to \~1M tokens**. ([Hugging Face][1], [Amazon Web Services, Inc.][2])
* Typical host limits today: **128K** (Groq), **512K** (Oracle OCI). If you’re deploying on a provider, their limit—not the model maximum—applies. ([Groq Console][3], [Oracle Docs][4])

**Modalities & languages**

* **Natively multimodal (early-fusion)**: text + images in, text out. ([Amazon Web Services, Inc.][2], [Hugging Face][1])
* **12 supported languages** for text (en, es, pt, fr, de, it, hi, id, vi, th, ar, tl). Image understanding is English-only per some hosts. ([Hugging Face][1], [Oracle Docs][4])

**Training & cutoff**

* Pretraining: \~**22T tokens** (Maverick). **Knowledge cutoff: Aug 2024.** ([Hugging Face][1])

**Licensing & weights**

* Released under the **Llama 4 Community License**; weights available (BF16) and **FP8** variants for efficient inference. ([Hugging Face][1])

**Runtime features (common across major hosts)**

* **Structured outputs** (JSON / JSON-Schema), **tool calling / function use**, and **multimodal chat**. ([Groq Console][3])

If you tell me where you plan to run it (self-host, Bedrock, Groq, OCI, HF Transformers), I’ll translate this into concrete context limits, quantization choices, and example configs.

[1]: https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct "meta-llama/Llama-4-Maverick-17B-128E-Instruct · Hugging Face"
[2]: https://aws.amazon.com/blogs/aws/llama-4-models-from-meta-now-available-in-amazon-bedrock-serverless/ "Llama 4 models from Meta now available in Amazon Bedrock serverless | AWS News Blog"
[3]: https://console.groq.com/docs/model/meta-llama/llama-4-maverick-17b-128e-instruct "Llama 4 Maverick - GroqDocs"
[4]: https://docs.oracle.com/en-us/iaas/Content/generative-ai/meta-llama-4-maverick.htm "Meta Llama 4 Maverick (New)"


Here’s a concise, deployment-oriented cheat-sheet for the three models.

### 1) How many parameters?

* **OpenAI GPT-OSS-120B** — **117B total**, **\~5.1B active per token** (MoE: 128 experts, 4 active). Context: **128K** tokens. ([OpenAI][1])
* **Meta Llama 4 Maverick** — **\~400B total**, **17B active per token** (MoE: 128 experts). Context: **\~1,000,000** tokens. ([Hugging Face][2])
* **DeepSeek-V3.1** — **671B total**, **37B active per token** (inherits V3 architecture). Context: **128K** tokens. ([Hugging Face][3], [arXiv][4])

### 2) Input / output prompt size (approx, in KB)

> Rule of thumb: \~**4 bytes/token** of UTF-8 text.
> **Max input+output** is bounded by the **context window**.

* **GPT-OSS-120B** (128K): **\~512 KB** total window. Your **input KB + output KB ≈ 512 KB**. (e.g., 20K-token prompt ≈ \~80 KB leaves \~432 KB for output). ([OpenAI][1])
* **Llama 4 Maverick** (1M): **\~4,000 KB (\~4 MB)** total window. **Input KB + output KB ≈ 4,000 KB**. ([Hugging Face][2])
* **DeepSeek-V3.1** (128K): **\~512 KB** total window. **Input KB + output KB ≈ 512 KB**. ([Hugging Face][3])

*(These are payload-size approximations; actual bytes vary with tokenizer/text. If a form needs hard numbers, provide the totals above and note they are “max combined input+output”.)*

### 3) Precision typically used for inference

* **GPT-OSS-120B** — Ships **natively quantized in MXFP4**; also runs in BF16/FP16 depending on runtime. Designed to **fit in 80 GB** with MXFP4. ([OpenAI][1])
* **Llama 4 Maverick** — Checkpoints available in **BF16** and **FP8**; Meta also provides **on-the-fly int4** quantization code. ([Hugging Face][2])
* **DeepSeek-V3.1** — Uses **FP8 (UE8M0) microscaling** on weights & activations; HF card also lists **BF16/F32** variants. ([Hugging Face][3])

### 4) Parallelism you’d use in an application (what to plan for)

* **GPT-OSS-120B**

  * **Single-GPU**: none needed (MXFP4 fits on 80 GB). ([OpenAI][1])
  * **Multi-GPU scaling** (if you shard or want higher throughput): **Data Parallel (DP)** for replicas; **Tensor Parallel (TP)** and/or **Pipeline Parallel (PP)** to shard layers; MoE layers can use **Expert Parallel (EP)** if spreading experts across GPUs. (General MoE practice.) ([NVIDIA Docs][5])
* **Llama 4 Maverick**

  * FP8 weights fit on a **single H100 DGX host** (multi-GPU box). In practice you’ll map **experts across GPUs (EP)** and often combine with **TP**; add **PP** for very long sequences; use **DP** for throughput. ([Hugging Face][2], [NVIDIA Docs][5])
* **DeepSeek-V3.1**

  * Very large MoE (671B total): plan for **EP** (experts sharded across GPUs) plus **TP**; many stacks also enable **Sequence Parallel (SP)** when mixing EP+TP; add **PP** if needed and **DP** for replicas. (Architecture per V3 report; parallelism per NVIDIA docs.) ([arXiv][4], [NVIDIA Docs][5], [GitHub][6])

---

If you tell me the exact infra (GPU count/memory, framework like vLLM/DeepSpeed/Megatron, and desired throughput), I can turn this into a concrete parallelism/topology plan and memory budget for each model.

[1]: https://openai.com/index/introducing-gpt-oss/ "Introducing gpt-oss | OpenAI"
[2]: https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E "meta-llama/Llama-4-Maverick-17B-128E · Hugging Face"
[3]: https://huggingface.co/deepseek-ai/DeepSeek-V3.1 "deepseek-ai/DeepSeek-V3.1 · Hugging Face"
[4]: https://arxiv.org/abs/2412.19437 "[2412.19437] DeepSeek-V3 Technical Report"
[5]: https://docs.nvidia.com/megatron-core/developer-guide/latest/api-guide/moe.html?utm_source=chatgpt.com "Mixture of Experts package"
[6]: https://github.com/NVIDIA/Megatron-LM?utm_source=chatgpt.com "NVIDIA/Megatron-LM: Ongoing research training ..."
