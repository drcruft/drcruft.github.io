import streamlit as st
from typing import List, Dict

# Initialize the page configuration for a professional look
def init_page():
    st.set_page_config(
        page_title="LLM Chat UI",
        page_icon="ðŸ’¬",
        layout="wide",
        initial_sidebar_state="collapsed"
    )
    st.markdown(
        """
        <style>
        .chat-container {
            max-width: 800px;
            margin: auto;
        }
        </style>
        """,
        unsafe_allow_html=True
    )

# Render chat history stored in Session State
def render_chat(messages: List[Dict[str, str]]):
    st.markdown('<div class="chat-container">', unsafe_allow_html=True)
    for msg in messages:
        role = msg.get("role")
        content = msg.get("content")
        if role == "user":
            st.chat_message("user").markdown(content)
        else:
            st.chat_message("assistant").markdown(content)
    st.markdown('</div>', unsafe_allow_html=True)

# Placeholder function for LLM call
def call_llm(user_input: str) -> str:
    # TODO: Replace with actual LLM call
    return f"You said: {user_input}"

# Main application
def main():
    init_page()
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # Display existing chat history (oldest at top)
    render_chat(st.session_state.messages)

    # Chat input at the bottom
    user_input = st.chat_input("Type your message here...")
    if user_input:
        # Append user's message immediately
        st.session_state.messages.append({"role": "user", "content": user_input})
        st.chat_message("user").markdown(user_input)

        # Show a placeholder assistant message while generating
        assistant_placeholder = st.chat_message("assistant")
        assistant_placeholder.markdown("...")

        # Call the LLM and update placeholder
        llm_response = call_llm(user_input)
        assistant_placeholder.markdown(llm_response)

        # Append assistant's response to history
        st.session_state.messages.append({"role": "assistant", "content": llm_response})

if __name__ == "__main__":
    main()
