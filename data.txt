## cruft

import os
import json
from crewai import LLM
from tools.splunk_search_tool import SplunkSearchTool

# 1) Instantiate the Splunk helper
splunk_tool = SplunkSearchTool(
    host=os.getenv("SPLUNK_HOST",   "splunk.example.com"),
    port=int(os.getenv("SPLUNK_PORT","8089")),
    username=os.getenv("SPLUNK_USERNAME","admin"),
    password=os.getenv("SPLUNK_PASSWORD","changeme"),
)

# 2) Build the vLLM‐style tool wrapper
splunk_tool_def = {
    "type": "function",
    "function": {
        "name":        splunk_tool.name,
        "description": splunk_tool.description,
        "parameters": {
            "type":       "object",
            "properties": {
                "query":    {"type":"string","description":"The SPL query to run"},
                "earliest": {"type":"string","description":"Earliest time, e.g. '-15m'"},
                "latest":   {"type":"string","description":"Latest time, e.g. 'now'"}
            },
            "required": ["query"]
        }
    }
}

# 3) Point at your local vLLM server
llm = LLM(
    model="command-r",
    api_key="not_used",
    base_url="http://127.0.0.1:8000/v1",
)

def ask(prompt: str) -> str:
    # **Here** is where we tell vLLM about our tool
    msg = llm.call(
        messages=[{"role":"user", "content": prompt}],
        tools=[splunk_tool_def],    # <— tool schema goes here
        tool_choice="auto"          # <— ask vLLM to pick & call it
    )

    # dispatch if it chose our function
    if hasattr(msg, "tool_calls") and msg.tool_calls:
        tc = msg.tool_calls[0]
        if tc.name == splunk_tool.name:
            args = tc.args
            return splunk_tool._run(**args)

    # otherwise, normal chat reply
    return msg.content

if __name__ == "__main__":
    q = input("You: ")
    print("Assistant:", ask(q))

