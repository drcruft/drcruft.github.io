1

Here’s the quick rundown on **DeepSeek-V3.1**.

**Parameter sizes**

* **671B total parameters, \~37B active per token** (MoE). Same as V3.&#x20;
* Architecture unchanged from V3: **DeepSeekMoE** + **Multi-head Latent Attention (MLA)**; each MoE layer has **1 shared + 256 routed experts**, with **top-8** experts active per token.&#x20;

**Context & modes**

* **128K context window** for both the “chat” (non-thinking) and “reasoner” (thinking) variants. ([DeepSeek API Docs][1])
* **Hybrid “Think / Non-Think” in one model**—you switch modes via the chat template; V3.1-Think is faster than R1-0528 for reaching answers. ([DeepSeek API Docs][2], [Hugging Face][3])

**What’s new/featured in V3.1**

* **Improved tool/agent skills** (better SWE/Terminal-Bench, multi-step tasks); **faster thinking efficiency**. ([DeepSeek API Docs][2])
* **Function calling**: supported in **non-thinking** (“deepseek-chat”) mode; **JSON output** supported in both modes. ([DeepSeek API Docs][1])
* **Two-phase long-context training** on top of V3: \~**630B tokens** for the 32K phase and **209B tokens** for the 128K phase; updated tokenizer & chat template. ([Hugging Face][3])
* **FP8 microscaling (UE8M0)** for weights & activations to improve efficiency/compatibility. ([Hugging Face][3])

**Carry-overs from V3 you still get in V3.1**

* **Auxiliary-loss-free load balancing** and **multi-token prediction** (used to speed up decoding), plus FP8 support in training.&#x20;

If you want, tell me your use case (chatbot, agent, long-doc QA, coding, etc.) and I’ll map you to the right mode + features.

[1]: https://api-docs.deepseek.com/quick_start/pricing "Models & Pricing | DeepSeek API Docs"
[2]: https://api-docs.deepseek.com/news/news250821 "DeepSeek-V3.1 Release | DeepSeek API Docs"
[3]: https://huggingface.co/deepseek-ai/DeepSeek-V3.1 "deepseek-ai/DeepSeek-V3.1 · Hugging Face"


Here’s the skinny on **OpenAI gpt-oss-120b** (aka “GPT-OSS 120B”):

* **Size & architecture**

  * **117B total params** (marketed as “120B”), **Mixture-of-Experts** transformer. **128 experts/layer**, **4 active per token** → about **5.1B active params** per token. **36 layers.** ([OpenAI][1])
  * **Attention & pos. enc.**: alternating dense + locally banded sparse attention, **grouped multi-query attention (group=8)**, **RoPE**. ([OpenAI][1])
  * **Context window:** **128K tokens.** ([OpenAI][1])

* **Tokenizer & format**

  * Uses the **o200k\_harmony** tokenizer and expects the **harmony response format**. ([OpenAI][1])

* **Deployment & memory**

  * **Native MXFP4 quantization**; runs on a **single 80 GB GPU** (e.g., H100/MI300X). Weights available on **Hugging Face**. ([OpenAI][1], [Hugging Face][2])

* **License**

  * **Apache-2.0** open-weight release. ([OpenAI][1], [Hugging Face][2])

* **Features**

  * **Reasoning levels** (low/medium/high) to trade off speed vs. accuracy. ([OpenAI][1], [Hugging Face][2])
  * **Tool use / agentic skills**: function calling, structured outputs, web browsing, and Python tool execution supported in the provided tool environments. ([OpenAI][1], [Hugging Face][2])
  * **Chain-of-thought access** for developers (OpenAI advises not to show CoT to end-users). ([OpenAI][1], [Hugging Face][2])
  * **Fine-tuning:** supported; **120B can be fine-tuned on a single H100 node.** ([Hugging Face][2])

If you tell me your use case (local inference, RAG, agents, etc.), I can suggest practical configs (reasoning level, quantization, context settings) for gpt-oss-120b.

[1]: https://openai.com/index/introducing-gpt-oss/ "Introducing gpt-oss | OpenAI"
[2]: https://huggingface.co/openai/gpt-oss-120b "openai/gpt-oss-120b · Hugging Face"


Here’s the essentials on **Meta Llama 4 “Maverick”**.

**Parameter sizes (MoE)**

* **17B activated params** per token, **\~400B total** across **128 experts** (Mixture-of-Experts). ([Hugging Face][1], [Amazon Web Services, Inc.][2])

**Context window (model-native vs. host limits)**

* Model-native: **up to \~1M tokens**. ([Hugging Face][1], [Amazon Web Services, Inc.][2])
* Typical host limits today: **128K** (Groq), **512K** (Oracle OCI). If you’re deploying on a provider, their limit—not the model maximum—applies. ([Groq Console][3], [Oracle Docs][4])

**Modalities & languages**

* **Natively multimodal (early-fusion)**: text + images in, text out. ([Amazon Web Services, Inc.][2], [Hugging Face][1])
* **12 supported languages** for text (en, es, pt, fr, de, it, hi, id, vi, th, ar, tl). Image understanding is English-only per some hosts. ([Hugging Face][1], [Oracle Docs][4])

**Training & cutoff**

* Pretraining: \~**22T tokens** (Maverick). **Knowledge cutoff: Aug 2024.** ([Hugging Face][1])

**Licensing & weights**

* Released under the **Llama 4 Community License**; weights available (BF16) and **FP8** variants for efficient inference. ([Hugging Face][1])

**Runtime features (common across major hosts)**

* **Structured outputs** (JSON / JSON-Schema), **tool calling / function use**, and **multimodal chat**. ([Groq Console][3])

If you tell me where you plan to run it (self-host, Bedrock, Groq, OCI, HF Transformers), I’ll translate this into concrete context limits, quantization choices, and example configs.

[1]: https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct "meta-llama/Llama-4-Maverick-17B-128E-Instruct · Hugging Face"
[2]: https://aws.amazon.com/blogs/aws/llama-4-models-from-meta-now-available-in-amazon-bedrock-serverless/ "Llama 4 models from Meta now available in Amazon Bedrock serverless | AWS News Blog"
[3]: https://console.groq.com/docs/model/meta-llama/llama-4-maverick-17b-128e-instruct "Llama 4 Maverick - GroqDocs"
[4]: https://docs.oracle.com/en-us/iaas/Content/generative-ai/meta-llama-4-maverick.htm "Meta Llama 4 Maverick (New)"
